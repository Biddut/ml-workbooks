{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to Start & Submit the Titanic Project\n\nHere i tried my best to cover each & every aspect,i belive you definetly not struck at any point & concept.\nAlso accuracy score at the end you will get 89% by considering all possibilities.If you find deep satisfaction then please\n### upvote & share this notebook."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"from IPython.display import YouTubeVideo               # PLEASE see the COMMENTS\nYouTubeVideo('8yZMXCaFshs', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's understand some basic concepts first before starting the projct"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F8507add53d0124ba15839ef965970306%2FIMG_20201016_131437.jpg?generation=1602836647135131&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F47383e24292228f12ce07cf09c548046%2FIMG_20201016_124826.jpg?generation=1602836698675142&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F55912e057e6842da4e5476c3ce628c52%2FIMG_20201016_130025.jpg?generation=1602836982531024&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F99cd2bb2385b9f79a9c45aa9502939dc%2FIMG_20201020_133152.jpg?generation=1603181216557322&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('xtOg44r6dsE', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom plotly.offline import init_notebook_mode,iplot\nimport plotly.graph_objects as go\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\nfrom IPython.display import Image\nImage(url=\"https://cdn.britannica.com/79/4679-050-BC127236/Titanic.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About Project\n\n### The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge,you to build a predictive model that answers the question:\n\n#### “what sorts of people were more likely to survive?” \n\nusing passenger data (ie name, age, gender, socio-economic class, etc).\n\n**Overview**\n\nThe data has been split into two groups:\n\ntraining set (train.csv) & test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n**Data Dictionary**\n\nVariable\tDefinition\t    Key\n\nsurvival\tSurvival\t    0 = No, 1 = Yes\n\npclass\t    Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex\t        Sex\t\n\nAge\t        Age in years\t\n\nsibsp\t   # of siblings / spouses aboard the Titanic\n\nparch\t   # of parents / children aboard the Titanic\n\nticket\t   Ticket number\t\n\nfare\t   Passenger fare\t\n\ncabin\t   Cabin number\t\n\nembarked   Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes\n\npclass: A proxy for socio-economic status (SES)\n\n1st = Upper\n\n2nd = Middle\n\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fiancés were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{},"cell_type":"markdown","source":"#### Importing Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ngender_submission=pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Very short Explanation\n\n**Here's the little introduction of graphs when & why which graph is appropriate to plt**"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F5ac3a43a0b2d38abfa1b5a08a8790ca2%2FIMG_20201020_132614.jpg?generation=1603181303434393&alt=media)"},{"metadata":{},"cell_type":"markdown","source":"# Let's visualize all in one\n\nthen alternatively we plot separtely each graph\n**********\n**Correlation Between The Features**"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Heatmap\nsns.heatmap(train.corr(),   #data.corr()-->correlation matrix\n            annot=True,\n            cmap='RdYlGn',\n            linewidths=0.2)\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df=train\ndf = df.drop(['Ticket','Cabin'], axis=1)\ndf = df.dropna()                            # Remove NaN values\nfig = plt.figure(figsize=(18,6), dpi=1600)  #specifies the parameters of our graphs\nalpha=alpha_scatterplot = 0.2 \nalpha_bar_chart = 0.55\n\nax1 = plt.subplot2grid((2,3),(0,0))         # lets us plot many diffrent shaped graphs together                \ndf.Survived.value_counts().plot(kind='bar', # plots a bar graph of those who surived vs those who did not.\n                                alpha=alpha_bar_chart)\n# this nicely sets the margins in matplotlib to deal with a recent bug 1.3.1\nax1.set_xlim(-1, 2)                         # puts a title on our graph\nplt.title(\"Distribution of Survival, (1 = Survived)\")    \n\nplt.subplot2grid((2,3),(0,1))\n\nplt.scatter(df.Survived, df.Age,            # sets the y axis lable\n            alpha=alpha_scatterplot)\nplt.ylabel(\"Age\")                           # formats the grid line style of our graphs \nplt.grid(b=True, which='major', axis='y')  \nplt.title(\"Survival by Age,  (1 = Survived)\")\n\nax3 = plt.subplot2grid((2,3),(0,2))\ndf.Pclass.value_counts().plot(kind=\"barh\",\n                              alpha=alpha_bar_chart)\nax3.set_ylim(-1, len(df.Pclass.value_counts()))\nplt.title(\"Class Distribution\")\n\nplt.subplot2grid((2,3),(1,0), colspan=2)\n\n# plots a kernel density estimate of the subset of the 1st class passangers's age\ndf.Age[df.Pclass == 1].plot(kind='kde')    \ndf.Age[df.Pclass == 2].plot(kind='kde')\ndf.Age[df.Pclass == 3].plot(kind='kde')\n\nplt.xlabel(\"Age\")                              # plots an axis lable\nplt.title(\"Age Distribution within classes\")\nplt.legend(('1st Class', '2nd Class',          # sets our legend for our graph.\n            '3rd Class'),loc='best') \n\nax5 = plt.subplot2grid((2,3),(1,2))\ndf.Embarked.value_counts().plot(kind='bar',\n                                alpha=alpha_bar_chart)\nax5.set_xlim(-1, len(df.Embarked.value_counts()))\nplt.title(\"Passengers per boarding location\")   # specifies the parameters of our graphs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets first built a model in a one step with high accuracy then i will explain all in detail**\n\nfor this please go to my another notebook\nhttps://www.kaggle.com/vik2012kvs/titanic-87-5-accuracy\n\ni made this notebook separately for better understanding"},{"metadata":{},"cell_type":"markdown","source":"# Detail Explanation"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Visualize the count of number of survivors\nsns.countplot(train['Survived'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize the count of survivors for the columns who, sex, pclass, sibsp, parch, and embarked.\nFrom the charts below, we can see that a man (a male 18 or older) is not likely to survive from the chart who.\nFemales are most likely to survive from the chart sex.\nThird class is most likely to not survive by chart pclass.\nIf you have 0 siblings or spouses on board, you are not likely to survive according to chart sibsp.\nIf you have 0 parents or children on board, you are not likely to survive according to the parch chart.\nIf you embarked from Southampton (S), you are not likely to survive according to the embarked chart"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"df=train\n\n#labels\nlab = df[\"Survived\"].value_counts().keys().tolist()\n#values\nval = df[\"Survived\"].value_counts().values.tolist()\ntrace = go.Pie(labels=lab, \n                values=val, \n                marker=dict(colors=['red']), \n                # Seting values to \n                hoverinfo=\"value\"\n              )\ndata = [trace]\n\n#layout: you can plot title, x and y axis titles or show legends \nlayout = go.Layout(title=\"Survived Distribution\")                #set title\n \n#figure: when you want to show on a graph, it takes the defined data and layout parameters\n\nfig = go.Figure(data = data,layout = layout)\n\niplot(fig)\n \n###############\n\n#matplot\n\ntitle = \"pie chart\"\nplt.pie(val, labels=lab)\nplt.axis('equal')\nplt.title(title)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Look at survival rate by sex\ntrain.groupby('Sex')[['Survived']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Plot the survival rate of each class.\nsns.barplot(x='Pclass', y='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at the survival rate by sex and class.\nFrom the pivot table below, we see that females in first class had a survival rate of about 96.8%,\nmeaning the majority of them survived.\nMales in third class had the lowest surviva****l rate at about 13.54%, meaning the majority of them did not survive."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Look at survival rate by sex and class\ntrain.pivot_table('Survived', index='Sex', columns='Pclass')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Look at survival rate by sex and class visually\ntrain.pivot_table('Survived', index='Sex', columns='Pclass').plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Look at survival rate by sex, age and class\nage = pd.cut(train['Age'], [0, 18, 80])\ntrain.pivot_table('Survived', ['Sex', age], 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Plot the Prices Paid Of Each Class\nimport matplotlib.pyplot as plt\nplt.scatter(train['Fare'], train['Pclass'],  color = 'purple', label='Passenger Paid')\nplt.ylabel('Class')\nplt.xlabel('Price / Fare')\nplt.title('Price Of Each Class')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize=(10,10))\nsns.countplot('Sex',hue='Survived',data=train, palette='RdBu_r')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#train.groupby('Sex').Survived.mean().plot(kind='bar')\nsns.barplot(x='Sex', y='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"tab = pd.crosstab(train['Pclass'], train['Sex'])\nprint (tab)\n\ntab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\nplt.xlabel('Pclass')\nplt.ylabel('Percentage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"sns.catplot('Sex', 'Survived', hue='Pclass', height=4, aspect=2, data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, it can be seen that:\n\nWomen from 1st and 2nd Pclass have almost 100% survival chance.\nMen from 2nd and 3rd Pclass have only around 10% survival chance."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"survived = train[train['Survived'] == 1]\nnot_survived = train[train['Survived'] == 0]\n\nprint (\"Survived: %i (%.1f%%)\"%(len(survived), float(len(survived))/len(train)*100.0))\nprint (\"Not Survived: %i (%.1f%%)\"%(len(not_survived), float(len(not_survived))/len(train)*100.0))\nprint (\"Total: %i\"%len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pclass vs. Survival\nHigher class passengers have better survival chance"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Pclass.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.groupby('Pclass').Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#train.groupby('Pclass').Survived.mean().plot(kind='bar')\nsns.barplot(x='Pclass', y='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"y=[]\nfare = []\nfor i in list(df['Pclass'].unique()):\n    result = df[df['Pclass']==i]['Age'].mean()\n    fares = df[df['Pclass']==i]['Fare'].mean()\n    y.append(result)\n    fare.append(fares)\n \n#defining data\ntrace = go.Bar(x = list(df['Pclass'].unique()),y=y,marker=dict(color=fare,colorscale='Viridis',showscale=True),text = fare)\ndata=[trace]\n#defining layout\nlayout = go.Layout(title='Age/Fare vs Pclass Bar Chart',xaxis=dict(title='Pclass'),yaxis=dict(title='Age'),hovermode='closest')\n#defining figure and plotting\nfigure = go.Figure(data=data,layout=layout)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#Pclass, Sex & Embarked vs. Survival\nsns.catplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, it can be seen that:\n\nAlmost all females from Pclass 1 and 2 survived.\nFemales dying were mostly from 3rd Pclass.\nMales from Pclass 1 only have slightly higher survival chance than Pclass 2 and 3."},{"metadata":{},"cell_type":"markdown","source":"**All in one**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Embarked vs. Survived\ntrain.Embarked.value_counts()\ntrain.groupby('Embarked').Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#train.groupby('Embarked').Survived.mean().plot(kind='bar')\nsns.barplot(x='Embarked', y='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Parch vs. Surviva\ntrain.Parch.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.groupby('Parch').Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#train.groupby('Parch').Survived.mean().plot(kind='bar')\nsns.barplot(x='Parch', y='Survived',ci=None, data=train) # ci=None will hide the error bar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#SibSp vs. Survival\ntrain.SibSp.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.groupby(\"SibSp\").Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"#train.groupby('SibSp').Survived.mean().plot(kind='bar')\nsns.barplot(x='SibSp', y='Survived', ci=None, data=train) # ci=None will hide the error bar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,7))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nsns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax1)\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax2)\nsns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train, split=True, ax=ax3)\n\ntotal_survived = train[train['Survived']==1]\ntotal_not_survived = train[train['Survived']==0]\nmale_survived = train[(train['Survived']==1) & (train['Sex']==\"male\")]\nfemale_survived = train[(train['Survived']==1) & (train['Sex']==\"female\")]\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"male\")]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']==\"female\")]\n\nplt.figure(figsize=[15,5])\nplt.subplot(111)\nsns.distplot(total_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(total_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Age')\n\nplt.figure(figsize=[15,5])\n\nplt.subplot(121)\nsns.distplot(female_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(female_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Female Age')\n\nplt.subplot(122)\nsns.distplot(male_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='blue')\nsns.distplot(male_not_survived['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel='Male Age')\n\nplt.figure(figsize=[10,10])\nsns.distplot(train['Age'].dropna().values, bins=range(0,17), kde=False, color=\"#007598\")\nsns.distplot(train['Age'].dropna().values, bins=range(16, 33), kde=False, color=\"#7B97A0\")\nsns.distplot(train['Age'].dropna().values, bins=range(32, 49), kde=False, color=\"#06319B\")\nsns.distplot(train['Age'].dropna().values, bins=range(48,65), kde=False, color=\"#007598\")\nsns.distplot(train['Age'].dropna().values, bins=range(64,81), kde=False, color=\"#000000\",\n            axlabel='Age')\nplt.show()\n\ntrain['Age_Category'] = pd.cut(train['Age'],\n                        bins=[0,16,32,48,64,81])\nplt.subplots(figsize=(10,10))\nsns.countplot('Age_Category',hue='Survived',data=train, palette='RdBu_r')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"1st Pclass has very few children as compared to other two classes. 1st Plcass has more old people as compared to other two classes. Almost all children (between age 0 to 10) of 2nd Pclass survived. Most children of 3rd Pclass survived. Younger people of 1st Pclass survived as compared to its older people. From Sex violinplot, we can see that:\n\nMost male children (between age 0 to 14) survived. Females with age between 18 to 40 have better survival chance"},{"metadata":{},"cell_type":"markdown","source":"From the above figures, we can see that:\n\nCombining both male and female, we can see that children with age between 0 to 5 have better chance of survival.\nFemales with age between \"18 to 40\" and \"50 and above\" have higher chance of survival.\nMales with age between 0 to 14 have better chance of survival.\n\nCorrelating Features\nHeatmap of Correlation between different features:\n\nPositive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n\nNegative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature."},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.5, square=False, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Extraction**\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.\n\n**Name Feature**\nLet's first extract titles from Name column."},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"from IPython.display import Image\nImage(url=\"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/5095eabce4b06cb305058603/5095eabce4b02d37bef4c24c/1352002236895/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F9eb0805dd47e9bf691bbba4ef0c4a133%2FIMG_20201017_142052.jpg?generation=1602924793629125&alt=media)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.crosstab(train['Title'], train['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#The number of passengers with each Title is shown above.We now replace some less common titles with the name \"Other\".\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', \\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#After that, we convert the categorical Title values into numeric form.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Sex Feature-We convert the categorical value of Sex into numeric. We represent 0 as female and 1 as male.\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embarked Feature\nThere are empty values for some rows for Embarked column. The empty values are represented as \"nan\" in below list.\n#Let's check the number of passengers for each Embarked category.\n"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Embarked.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find that category \"S\" has maximum passengers. Hence, we replace \"nan\" values with \"S\"."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#We now convert the categorical value of Embarked into numeric. We represent 0 as S, 1 as C and 2 as Q.\nfor dataset in train_test_data:\n    #print(dataset.Embarked.unique())\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Age Feature\nWe first fill the NULL values of Age with a random number between (mean_age - std_age) and (mean_age + std_age).\n\nWe then create a new column named AgeBand. This categorizes age into 5 different age range."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Now, we map Age according to AgeBand.\n\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"##Fare Feature-Replace missing Fare values with the median of Fare.\n\nfor dataset in train_test_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n    \n#Create FareBand. We divide the Fare into 4 category range.\n\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\nprint (train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean())\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Map Fare according to FareBand\n\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"##SibSp & Parch Feature-Combining SibSp & Parch feature, we create a new feature named FamilySize.\n\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1\n\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with Feature Selection/Engineering. Now, we are ready to train a classifier with our feature set.\n\n#  Classification & Accuracy\n\nDefine training and testing set"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train = train.drop(['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'PassengerId', 'Age_Category', 'FareBand'], axis=1)\ntrain['Age'] = train['Age'].fillna(2)\ntrain['Age'] = train['Age'].astype(int)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine learning Tutorial\n\n**Download the pdf first which contains concepts all codes**\n\n1.Python\n\nhttps://1drv.ms/u/s!AsZgYh4LWHp-gbMrSNPbwvdYjTflHA?e=IkmNvE\n\n2.Machine Learning\n\nhttps://1drv.ms/u/s!AsZgYh4LWHp-gbNEdqIk3Iow2W5oZg?e=GAGqVz\n\n3.AI Deep Learning\n\nhttps://1drv.ms/u/s!AsZgYh4LWHp-gbNDC5_OWQ_LQwEs5Q?e=zvweWZ\n\n4.Natural Language Processing\n\nhttps://1drv.ms/u/s!AsZgYh4LWHp-gbM5O4NFj2_1se_sKg?e=xpjP63\n\n**All books of latestes edition of Probability,Statistics,Machine Learning,Data Science**\n\n1.Booklists provided by MIT (most of them are free)\n\nhttps://drive.google.com/file/d/1XRCbtNz2k-H5b_CXO-ZSAxnoD6S2NZTF/view?usp=drivesdk\n\n2.Data Science Books(Probability, Linear Algebra, Statistics, Data Analytics….)\n\nhttps://mega.nz/folder/0iZFXCbA#Rwh3Km42_YaRvgY_NOAvWw\n\nhttps://mega.nz/folder/g2BRhaDJ#v2XWSegTk3sH6ZcLPNG-WA\n\n3.Python & Machine Learning books(Programming, Applied Statistics with R…Etc)\nhttps://mega.nz/folder/NmQRlaBa#0FKTDkkHYBmkSmcEu0kGoQ"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('GwIo3gDZCVQ', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"Image(url=\"https://cdn.cnn.com/cnnnext/dam/assets/170321134751-titanic-tour-1-full-169.jpg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELS\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F071b7bebf9b164bc2c0e22e5e6fe4ddd%2FIMG_20201017_141847.jpg?generation=1602924582687966&alt=media)\n\n**There are many classifying algorithms present**.\nAmong them, we choose the following Classification algorithms for our problem:\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F2b860f5d02e4a737c13cc7b35335a35a%2FIMG_20201016_123347.jpg?generation=1602836745520798&alt=media)\n****\nLogistic Regression\nNow we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n****\nLogistic Regression\n\nKNN\n\nSupport Vector Machines\n\nNaive Bayes classifier\n\nDecision Tree\n\nRandom Forrest\n\nLinear Discriminant Analysis\n\nAda Boost Classifier\n\nGradient Boosting Classifier\n****\nAnd also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix\n\n**Training and testing procedure:**\n\nFirst, we train these classifiers with our training data.\n\nAfter that, using the trained classifier, we predict the Survival outcome of test data.\n\nFinally, we calculate the accuracy score (in percentange) of the trained classifier.\n\nPlease note: that the accuracy score is generated based on our training dataset"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split      #for split the data\nfrom sklearn.metrics import accuracy_score                #for accuracy_score\nfrom sklearn.model_selection import KFold                 #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score       #score evaluation\nfrom sklearn.model_selection import cross_val_predict     #prediction\nfrom sklearn.metrics import confusion_matrix              #for confusion matrix\nall_features = train.drop(\"Survived\",axis=1)\nTargeted_feature = train[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,\n                                                 Targeted_feature,\n                                                 test_size=0.3,\n                                                 random_state=42)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LogisticRegression"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F8317bf71a6526b91f1c0184862041616%2FIMG_20201016_115554.jpg?generation=1602837044076161&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2Fd95e090dfb9cca14785dfb0073cb3b41%2FIMG_20201016_115635.jpg?generation=1602837075684419&alt=media)\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F7ad07dc75988e7715413aa8db82daad1%2FIMG_20201016_120021.jpg?generation=1602837246672661&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('yIYKR4sgzI8', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression           # Logistic Regression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Logistic Regression is',\n      round(accuracy_score(prediction_lr,y_test)*100,2))\n\nkfold = KFold(n_splits=10, random_state=22)                     # k=10, split the data into 10 equal parts\nresult_lr=cross_val_score(model,all_features,\n                          Targeted_feature,cv=10,\n                          scoring='accuracy')\n\nprint('The cross validated score for Logistic REgression is:',\n      round(result_lr.mean()*100,2))\n\ny_pred = cross_val_predict(model,all_features,\n                           Targeted_feature,cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),\n            annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('VCJdg7YBbAQ', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F94d218311177e413b0f7552a397d1f76%2FIMG_20201016_120231.jpg?generation=1602837459605327&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('J4Wdy0Wc_xQ', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Part-2\n\nhttps://www.youtube.com/watch?v=2xudPOBz-vs\n\nPart-3\n\nhttps://www.youtube.com/watch?v=jxuNLH5dXCs\n\nPart-4\n\nhttps://www.youtube.com/watch?v=StWY5QWMXCw"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion='gini',\n                               n_estimators=700,\n                             min_samples_split=10,\n                               min_samples_leaf=1,\n                             max_features='auto',\n                               oob_score=True,\n                             random_state=1,\n                               n_jobs=-1)\nmodel.fit(X_train,y_train)\nprediction_rm=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Random Forest Classifier is',\n      round(accuracy_score(prediction_rm,y_test)*100,2))\n\nkfold = KFold(n_splits=10,\n              random_state=22)        # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,\n                          all_features,\n                          Targeted_feature,\n                          cv=10,\n                          scoring='accuracy')\n\nprint('The cross validated score for Random Forest Classifier is:',\n      round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix', y=1.05, size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machines\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F0888580881303be588284d7be0cb90ec%2FIMG_20201016_120134.jpg?generation=1602837312996663&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('TtKF996oEl8', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\n\nprint('The accuracy of the Support Vector Machines Classifier is',\n      round(accuracy_score(prediction_svm,y_test)*100,2))\n\nkfold = KFold(n_splits=10,                                       # k=10, split the data into 10 equal parts\n              random_state=22)\n\nresult_svm=cross_val_score(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for Support Vector Machines Classifier is:',\n      round(result_svm.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05, size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classifier\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F4f1908509ec7c2da5ee664130c7770ef%2FIMG_20201016_120048.jpg?generation=1602837281910972&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('HVXime0nQeI', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors = 4)\nmodel.fit(X_train,y_train)\nprediction_knn=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the K Nearst Neighbors Classifier is',\n      round(accuracy_score(prediction_knn,\n                           y_test)*100,2))\n\nkfold = KFold(n_splits=10,                               # k=10, split the data into 10 equal parts\n              random_state=22)\n\nresult_knn=cross_val_score(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for K Nearest Neighbors Classifier is:',\n      round(result_knn.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),\n            annot=True,fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05,\n          size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Naive Bayes"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('O2L2Uv9pdDA', width=800, height=550)  #Use this to understand Naive Bayes\nYouTubeVideo('H3EjCKtlVog', width=800, height=550)  #Use this to understand Gaussian Naive Bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gaussian Naive Bayes Classifier is',\n      round(accuracy_score(prediction_gnb,y_test)*100,2))\n\nkfold = KFold(n_splits=10,                 # k=10, split the data into 10 equal parts\n              random_state=22)\nresult_gnb=cross_val_score(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for Gaussian Naive Bayes classifier is:',\n      round(result_gnb.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,\n                             y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05,\n          size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F5561157%2F5275dc42793c47c8940d526e75fa1715%2FIMG_20201016_120204.jpg?generation=1602837419903922&alt=media)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('7VeUPuFGJHk', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('qDcl-FRnwSU', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmodel= DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,\n                              min_samples_leaf=1,\n                             max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the DecisionTree Classifier is',\n      round(accuracy_score(prediction_tree,\n                           y_test)*100,2))\n\nkfold = KFold(n_splits=10,                           # k=10, split the data into 10 equal parts\n          random_state=22)\nresult_tree=cross_val_score(model,\n                            all_features,\n                            Targeted_feature,\n                            cv=10,\n                            scoring='accuracy')\n\nprint('The cross validated score for Decision Tree classifier is:',\n      round(result_tree.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,\n                             y_pred),\n                            annot=True,\n                            fmt='3.0f',\n                            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n                      y=1.05,\n                     size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AdaBoost"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('LsK-xG1cLYA', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nmodel= AdaBoostClassifier()\nmodel.fit(X_train,y_train)\nprediction_adb=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the AdaBoostClassifier is',\n      round(accuracy_score(prediction_adb,y_test)*100,2))\n\nkfold = KFold(n_splits=10,          # k=10, split the data into 10 equal parts\n              random_state=22)\n\nresult_adb=cross_val_score(model,all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for AdaBoostClassifier is:',\n      round(result_adb.mean()*100,2))\n\ny_pred = cross_val_predict(model,all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,\n                             y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05,\n          size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('azXCzI57Yfc', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodel= LinearDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\nprediction_lda=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the LinearDiscriminantAnalysis is',\n      round(accuracy_score(prediction_lda,\n                           y_test)*100,2))\n\nkfold = KFold(n_splits=10,                     # k=10, split the data into 10 equal parts\n              random_state=22)\nresult_lda=cross_val_score(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for AdaBoostClassifier is:',\n      round(result_lda.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\n\nsns.heatmap(confusion_matrix(Targeted_feature,\n                             y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05,\n          size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Gradient Boosting Classifier"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('s3VmuVPfu0s', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel= GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc=model.predict(X_test)\n\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gradient Boosting Classifier is',\n      round(accuracy_score(prediction_gbc,y_test)*100,2))\n\nkfold = KFold(n_splits=10,               # k=10, split the data into 10 equal parts\n              random_state=22)\n\nresult_gbc=cross_val_score(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10,\n                           scoring='accuracy')\n\nprint('The cross validated score for AdaBoostClassifier is:',\n      round(result_gbc.mean()*100,2))\n\ny_pred = cross_val_predict(model,\n                           all_features,\n                           Targeted_feature,\n                           cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,\n                             y_pred),\n            annot=True,\n            fmt='3.0f',\n            cmap=\"summer\")\n\nplt.title('Confusion_matrix',\n          y=1.05,\n          size=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation\n\n**Now rank our evaluation of all the models to choose the best one for our problem**"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines',\n              'KNN',\n              'Logistic Regression', \n              'Random Forest',\n              'Naive Bayes',\n              'AdaBoostClassifier', \n              'Gradient Decent',\n              'Linear Discriminant Analysis', \n              'Decision Tree'],\n    'Score': [result_svm.mean(),\n              result_knn.mean(),\n               result_lr.mean(), \n               result_rm.mean(),\n              result_gnb.mean(),\n              result_adb.mean(), \n              result_gbc.mean(),\n              result_lda.mean(),\n              result_tree.mean()]})\n\nmodels.sort_values(by='Score',\n                   ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest & SVM classifier has a higher chance in correctly predicting dead passengers.**\n****\n# Hyper-Parameters Tuning\n\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning\n\nSo based on the above given acuracy result i will performance Grid search and random search for the SVM\n\nLDA\n\nLogistic Regression\n\nGradient Decent Classifier\n\nRandom Forest Classifier\n\nParameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\nn_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n\nn_estimators : Number of classification trees in your learning model ( set to 10 per default)\n\nmax_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n\nverbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n\nPlease check out the full description via the official Sklearn website. There you will find that there are a whole host of other useful parameters that you can play around with."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_X = train.drop(\"Survived\",\n                        axis=1)\n\ntrain_Y=train[\"Survived\"]\ntest_X  = test.drop(\"PassengerId\",\n                    axis=1).copy()\n\ntrain_X.shape, train_Y.shape, test_X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient boosting tunning"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('9HomdnM12o4', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.2,0.1] \n              }\n\nmodelf = GridSearchCV(model,\n                      param_grid = param_grid,\n                      cv=kfold,\n                      scoring=\"accuracy\",\n                      n_jobs= 4,\n                      verbose = 1)\n\nmodelf.fit(train_X,train_Y)\nmodelf.best_score_                                   # Best score\nmodelf.best_estimator_                               # Best Estimator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Best Model**"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"modelf.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier Parameters tunning"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"model = RandomForestClassifier()\nn_estim=range(100,1000,100)\n\nparam_grid = {\"n_estimators\" :n_estim}          # Search grid for optimal parameters\n\nmodel_rf = GridSearchCV(model,\n                        param_grid = param_grid, \n                        cv=5, scoring=\"accuracy\", \n                        n_jobs= 4, \n                        verbose = 1)\n\nmodel_rf.fit(train_X,\n             train_Y)\nprint(model_rf.best_score_)                        # Best score\nmodel_rf.best_estimator_                           #best estimator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis tunning"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodel =LinearDiscriminantAnalysis()\nparam_grid = {'tol':[0.001,0.01,.1,.2]}\n\nmodell = GridSearchCV(model,\n                      param_grid = param_grid, \n                      cv=5, \n                      scoring=\"accuracy\", \n                      n_jobs= 4, \n                      verbose = 1)\n\nmodell.fit(train_X,\n           train_Y)\nprint(modell.best_score_)                      # Best score\nmodell.best_estimator_                         # Best Estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":false},"cell_type":"code","source":"model= SVC()\nparam_grid = {'kernel': ['rbf','linear'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodelsvm = GridSearchCV(model,\n                        param_grid = param_grid, \n                        cv=5, scoring=\"accuracy\",\n                        n_jobs= 4, \n                        verbose = 1)\n\nmodelsvm.fit(train_X,\n             train_Y)\nprint(modelsvm.best_score_)                    # Best score\nmodell.best_estimator_                         # Best Estimatorprint(modelsvm.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply the Estimator which got from parameter tuning of Random Forest"},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True,\n                                       class_weight=None,\n                                       criterion='gini',\n                                       max_depth=None,\n                                       max_features='auto',\n                                       max_leaf_nodes=None,\n                                       min_impurity_decrease=0.0, \n                                       min_impurity_split=None,\n                                       min_samples_leaf=1,\n                                       min_samples_split=2,\n                                       min_weight_fraction_leaf=0.0, \n                                       n_estimators=400,\n                                       n_jobs=1,\n                                       oob_score=False,\n                                       random_state=None,\n                                       verbose=0,\n                                       warm_start=False)\n\nrandom_forest.fit(train_X, \n                  train_Y)\n\nrandom_forest.score(train_X,\n                    train_Y)\n\nacc_random_forest = round(random_forest.score(train_X, \n                                              train_Y) * 100, 2)\n\n\nprint(\"Important features\")\n\npd.Series(random_forest.feature_importances_,\n          train_X.columns).sort_values(ascending=True).plot.barh(width=0.6)\n\nprint('__'*30)\nprint(acc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For Complete Solution watch this video"},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"YouTubeVideo('I3FBJdiExcg', width=800, height=550)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"Image(url=\"https://farm5.staticflickr.com/4346/36769420926_f274314780_o.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import HTML\nimport base64\n\ndef create_download_link( submission, title = \"Download CSV file\", filename = \"Titanic Survival\"):  \n    csv = submission.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = f'<a target=\"_blank\">{title}</a>'\n    return HTML(html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For final submission use this**\n\n> submission = pd.DataFrame({\n>         \"PassengerId\": test[\"PassengerId\"],\n>         \"Survived\": Y_pred_rf})\n\nhere,i did one mistake so please omit it\n\nHappy Learning with **Prof. Andrew Ng** who had given the plus momentum of AI-ML with Carol E. Reiley(wife)\n\nComplete the course given below\n\nhttps://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\n\n# *Happy Learning*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}